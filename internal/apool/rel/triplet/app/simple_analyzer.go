package app

import (
	"context"
	"fmt"
	"io"
	"log"
	"os"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/dgraph-io/badger/v4"
	relapp "github.com/rlaaudgjs5638/chainAnalyzer/internal/apool/rel"
	"github.com/rlaaudgjs5638/chainAnalyzer/internal/apool/rel/triplet/infra"
	ropeapp "github.com/rlaaudgjs5638/chainAnalyzer/shared/dblib/ropedb/app"
	shareddomain "github.com/rlaaudgjs5638/chainAnalyzer/shared/domain"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/kafka"
)

// TripletAnalyzer ì¸í„°í˜ì´ìŠ¤ - í…ŒìŠ¤íŠ¸ìš©ê³¼ í”„ë¡œë•ì…˜ìš© ê³µí†µ ì¸í„°í˜ì´ìŠ¤
// ! ë‘ êµ¬í˜„ì²´ëŠ” ë°ì´í„° ì €ì¥ ë°©ì‹ê³¼ ìƒëª…ì£¼ê¸°ì—ì„œë§Œ ì°¨ì´ê°€ ìˆìŒ
type TripletAnalyzer interface {
	// ë¶„ì„ê¸° ìƒëª…ì£¼ê¸° ê´€ë¦¬
	Start(ctx context.Context) error
	Stop() error

	// íŠ¸ëœì­ì…˜ ì²˜ë¦¬
	ProcessTransaction(tx *shareddomain.MarkedTransaction) error
	ProcessTransactions(txs []*shareddomain.MarkedTransaction) error

	// ìƒíƒœ ì¡°íšŒ
	GetStatistics() map[string]any
	IsHealthy() bool
	GetChannelStatus() (usage int, capacity int)
	//ê·¸ë˜, ë§ë‹¤. ì¸í„°í˜ì´ìŠ¤ëŠ” ì´ë”°êµ¬ë¡œ ì“°ë©´ ì•ˆë˜ì§€/
	//ê·¼ë° ì´ê±° ê³ ì¹˜ë ¤ë©´ ë˜ ë¦¬íŒ©í† ë§ í•´ì•¼í•¨. ë˜!!!
	//ê·¸ê±´ ë‚˜ì¤‘ì— í•˜ìê³ .
	GraphDB() *badger.DB
	RopeDB() ropeapp.RopeDB
	GetRopeDBStats() map[string]any
	//TODO ì´ë”´ê±´ ë‹¹ì—°íˆ ê¸ˆì§€ì„. ì¶”í›„ ë¡œí”„DBê´€ë ¨ ì¸í„°í˜ì´ìŠ¤ ì‹¹ ì œê±°í•˜ê¸°
	// ë¦¬ì†ŒìŠ¤ ê´€ë¦¬
	io.Closer
}

// SimpleEOAAnalyzer ê°„ë‹¨í•œ EOA ë¶„ì„ê¸° êµ¬í˜„ì²´
// * í…ŒìŠ¤íŠ¸ìš©ê³¼ í”„ë¡œë•ì…˜ìš© ëª¨ë‘ ì§€ì›í•˜ëŠ” ê¸°ë³¸ êµ¬í˜„
type SimpleEOAAnalyzer struct {
	// Core domain components
	dualManager *DualManager

	// WorkerPool integration
	//ë‚´ë¶€ ì±„ë„ì„
	stopChannel  chan struct{}
	stopOnce     sync.Once
	shutdownOnce sync.Once
	wg           sync.WaitGroup

	// Transaction consumer (Kafka ê¸°ë°˜)
	batchMode bool // ë°°ì¹˜ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€

	// Configuration
	config *EOAAnalyzerConfig

	// Statistics (thread-safe atomic counters)
	stats SimpleAnalyzerStats

	infra   infra.TotalEOAAnalyzerInfra
	relPool *relapp.RelationPool
}

// SimpleAnalyzerStats ê°„ë‹¨í•œ ë¶„ì„ê¸° í†µê³„
type SimpleAnalyzerStats struct {
	TotalProcessed    int64
	SuccessCount      int64
	ErrorCount        int64
	DepositDetections int64
	GraphUpdates      int64
	WindowUpdates     int64
	DroppedTxs        int64
	StartTime         time.Time
}

// NewProductionEOAAnalyzer í”„ë¡œë•ì…˜ìš© ë¶„ì„ê¸° ìƒì„±
func NewProductionEOAAnalyzer(config *EOAAnalyzerConfig, ctx context.Context, relPool *relapp.RelationPool) (TripletAnalyzer, error) {
	infraStructure := NewInfraByConfig(config, ctx)
	return newSimpleAnalyzer(config, infraStructure, relPool)
}

// NewTestingEOAAnalyzer í…ŒìŠ¤íŠ¸ìš© ë¶„ì„ê¸° ìƒì„±
func NewTestingEOAAnalyzer(config *EOAAnalyzerConfig, ctx context.Context, relPool *relapp.RelationPool) (TripletAnalyzer, error) {
	infraStructure := NewInfraByConfig(config, ctx)
	return newSimpleAnalyzer(config, infraStructure, relPool)
}

// newSimpleAnalyzer ê³µí†µ ë¶„ì„ê¸° ìƒì„± ë¡œì§
func newSimpleAnalyzer(config *EOAAnalyzerConfig, infraStructure infra.TotalEOAAnalyzerInfra, relPool *relapp.RelationPool) (*SimpleEOAAnalyzer, error) {
	//ì „ì²´ EOAì¸í”„ë¼ì—ì„œ êº¼ë‚´ ì“°ëŠ” í˜•ì‹
	dualManagerInfra := infra.NewDualManagerInfra(infraStructure.GroundKnowledge, infraStructure.PendingRelationRepo)
	dualManager, _ := NewDualManager(*dualManagerInfra, relPool)

	log.Printf("ğŸ”„ DualManager with pending DB at: %s", config.PendingDBPath)
	log.Printf("ë“€ì–¼ ë§¤ë‹ˆì ¸ ì´ˆê¸°í™”. í˜„ì¬ cexì£¼ì†Œ ê°œìˆ˜: %d, ì˜ˆì‹œ:%s", len(dualManager.infra.GroundKnowledge.GetCEXAddresses()), dualManager.infra.GroundKnowledge.GetCEXAddresses()[0])
	analyzer := &SimpleEOAAnalyzer{
		infra:       infraStructure,
		dualManager: dualManager,
		stopChannel: make(chan struct{}),
		batchMode:   true, // ê¸°ë³¸ê°’: ë°°ì¹˜ ëª¨ë“œ í™œì„±í™”
		config:      config,
		stats: SimpleAnalyzerStats{
			StartTime: time.Now(),
		},
		relPool: relPool,
	}

	log.Printf("âœ… Simple EOA Analyzer created: %s", config.Name)

	log.Printf("âœ… SimpleAnalyzerì˜ DBë¥¼  RelationPoolë¡œ í¬ì¸íŒ…í•¨")
	return analyzer, nil
}
func (a *SimpleEOAAnalyzer) GraphDB() *badger.DB {
	if p, ok := a.relPool.RopeRepo.(infra.RawBadgerProvider); ok {
		return p.RawBadgerDB()
	}
	return nil
}

func (a *SimpleEOAAnalyzer) RopeDB() ropeapp.RopeDB {
	return a.relPool.RopeRepo
}

// Start ë¶„ì„ê¸° ì‹œì‘
func (a *SimpleEOAAnalyzer) Start(ctx context.Context) error {
	log.Printf("ğŸš€ Starting Simple Analyzer: %s", a.config.Name)

	// Consumer ì‹œì‘ (ë°°ì¹˜ ëª¨ë“œ or ë‹¨ê±´ ëª¨ë“œ)
	if a.batchMode && a.infra.BatchConsumer != nil {
		// ë°°ì¹˜ ëª¨ë“œ: ë°°ì¹˜ Consumer ì‹œì‘
		a.wg.Add(1)
		go a.batchConsumerWorker(ctx)
		log.Printf("ğŸš€ Batch consumer started")
	} else {
		log.Printf("ë‹¨ê±´ ì»¨ìŠˆë¨¸ëŠ” ê± ì§€ì› ìŒ.")
	}

	// í†µê³„ ë¦¬í¬í„° ì‹œì‘
	a.wg.Add(1)
	go a.statsReporter(ctx)

	log.Printf("âœ… Simple Analyzer started: %s (%d workers + kafka consumer)", a.config.Name, a.config.WorkerCount)

	// ì»¨í…ìŠ¤íŠ¸ ì·¨ì†Œ ë˜ëŠ” ì •ì§€ ì‹œê·¸ë„ ëŒ€ê¸°
	select {
	case <-ctx.Done():
		log.Printf("ğŸ›‘ Context cancelled: %s", a.config.Name)
	case <-a.stopChannel:
		log.Printf("ğŸ›‘ Stop signal received: %s", a.config.Name)
	}

	return a.shutdown()
}

// Stop ë¶„ì„ê¸° ì¤‘ì§€
func (a *SimpleEOAAnalyzer) Stop() error {
	a.stopOnce.Do(func() {
		close(a.stopChannel)
	})
	return nil
}

// ProcessTransaction íŠ¸ëœì­ì…˜ ì²˜ë¦¬ (non-blocking)
// TODO í˜„ì¬ ì´ ë¶€ë¶„ì—ì„œ ìŠ¤ë ˆë“œ ê°„ ì„±ëŠ¥ ì €í•˜ ë°œìƒí•¨. ìŠ¤ë ˆë“œ 1ê°œë‚˜ 16ê°œë‚˜ ë™ì¼ ì„±ëŠ¥ ë³´ì„
// TODO TxJobì˜ Do()ê°€ ì„œë¡œ ê²½í•© ë°œìƒ. íŒ¨ë‹‰ì€ ì•„ë‹ˆì§€ë§Œ, ì•”ë¬µì  ì„±ëŠ¥ ì €í•˜ ë°œìƒì¤‘
// TODO ë¬¸ì œì— ëŒ€í•œ ì§„ë‹¨ ë° ì¶”í›„ ê°œì„  ë°©ì•ˆì€ /debugì˜ upgrade_solution.mdì— ìì„¸íˆ ì ì–´ë†¨ìŒ.
func (a *SimpleEOAAnalyzer) ProcessTransaction(tx *shareddomain.MarkedTransaction) error {
	job := NewTransactionJob(tx, a, 0) // workerIDëŠ” ì›Œì»¤í’€ì—ì„œ ìë™ ê´€ë¦¬
	select {
	case a.infra.TxJobChannel <- job:
		return nil
	default:
		atomic.AddInt64(&a.stats.DroppedTxs, 1)
		return fmt.Errorf("channel full, dropped tx: %s", tx.TxID.String()[:8])
	}
}

// ProcessTransactions ë°°ì¹˜ íŠ¸ëœì­ì…˜ ì²˜ë¦¬
func (a *SimpleEOAAnalyzer) ProcessTransactions(txs []*shareddomain.MarkedTransaction) error {
	for _, tx := range txs {
		if err := a.ProcessTransaction(tx); err != nil {
			continue // ê°œë³„ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì²˜ë¦¬
		}
	}
	return nil
}

// transactionWorkerëŠ” ì´ì œ ì›Œì»¤í’€ì— ì˜í•´ ëŒ€ì²´ë¨ - í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
// ì‹¤ì œ ì‘ì—…ì€ TransactionJob.Do()ì—ì„œ ì²˜ë¦¬ë¨
// batchConsumerWorker ë°°ì¹˜ Consumer ì›Œì»¤ (ê³ ì„±ëŠ¥ ë°°ì¹˜ ì²˜ë¦¬)
func (a *SimpleEOAAnalyzer) batchConsumerWorker(ctx context.Context) {
	defer a.wg.Done()

	log.Printf("ğŸš€ Batch consumer worker started")

	for {
		select {
		case <-ctx.Done():
			log.Printf("ğŸ›‘ Batch consumer worker stopping (context)")
			return
		case <-a.stopChannel:
			log.Printf("ğŸ›‘ Batch consumer worker stopping (signal)")
			return
		default:
			// ë°°ì¹˜ ë©”ì‹œì§€ ì½ê¸° (ë¸”ë¡œí‚¹)
			messages, err := a.infra.BatchConsumer.ReadMessagesBatch(ctx)
			if err != nil {
				// Context cancellationì€ ì •ìƒì ì¸ ì¢…ë£Œ
				if ctx.Err() != nil {
					return
				}
				// ê¸°íƒ€ ì—ëŸ¬ëŠ” ë¡œê¹…í•˜ê³  ê³„ì†
				log.Printf("âš ï¸ Batch read error: %v", err)
				time.Sleep(100 * time.Millisecond) // ì—ëŸ¬ ì‹œ ì§§ì€ ëŒ€ê¸°
				continue
			}

			// ë°°ì¹˜ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
			if len(messages) == 0 {
				continue
			}

			// ë°°ì¹˜ ì²˜ë¦¬ (ì§„ì •í•œ ë°°ì¹­!)
			a.processTransactionParrell(messages)
		}
	}
}

// processTransactionParrell ë°°ì¹˜ ë©”ì‹œì§€ ì²˜ë¦¬ (ê³ íš¨ìœ¨)
func (a *SimpleEOAAnalyzer) processTransactionParrell(messages []kafka.Message[*shareddomain.MarkedTransaction]) {
	batchSize := len(messages)
	processedCount := atomic.LoadInt64(&a.stats.TotalProcessed)

	// ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ë¡œê¹… (ì²˜ìŒ ëª‡ ë°°ì¹˜ë§Œ)
	if processedCount < 500 {
		log.Printf("ğŸ“¦ Processing batch of %d messages (total processed: %d)", batchSize, processedCount)
	}

	transactions := make([]*shareddomain.MarkedTransaction, 0, batchSize)

	// 1. ë©”ì‹œì§€ì—ì„œ ì§ì ‘ íŠ¸ëœì­ì…˜ ì¶”ì¶œ (íŒŒì‹± ë¶ˆí•„ìš”!)
	for _, msg := range messages {
		if msg.Value != nil {
			transactions = append(transactions, msg.Value)
		} else {
			atomic.AddInt64(&a.stats.ErrorCount, 1)
		}
	}

	// 2. íŠ¸ëœì­ì…˜ ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ì²˜ë¦¬)
	for _, tx := range transactions {
		// ì›Œì»¤í’€ë¡œ ì‘ì—… ì „ë‹¬
		job := NewTransactionJob(tx, a, 0)
		select {
		case a.infra.TxJobChannel <- job:
			// ì„±ê³µ
		default:
			// ì±„ë„ì´ ê½‰ ì°¬ ê²½ìš° ì ì‹œ ì…ë ¥ ë©ˆì¶”ê¸°
			fmt.Printf("í˜„ì¬ EOA Analyzerì˜ ì›Œì»¤í’€ ì±„ë„ì´ ë‹¤ ë“¤ì–´ì°¼ìŒ. 0.1ì´ˆê°„ ì…ë ¥ì„ ë¸”ë¡œí‚¹í•¨.")
			time.Sleep(10 * time.Millisecond) // ë„ˆë¬´ ë¬´ê±°ìš´ ëŒ€ê¸°ëŠ” í”¼í•˜ê¸°
			// ì±„ë„ì´ ê°€ë“ ì°¬ ê²½ìš° ë“œë¡­
		}
	}

	// ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹… (ì²˜ìŒ ëª‡ ë°°ì¹˜ë§Œ)
	if processedCount < 500 {
		log.Printf("ğŸ“¦ Batch processed: %d messages â†’ %d transactions", batchSize, len(transactions))
	}
}

// processSingleTransaction ë©”ì„œë“œëŠ” TransactionJob.Do()ë¡œ ì´ë™ë¨
// í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‚­ì œ

// analyzeTransactionResult íŠ¸ëœì­ì…˜ ê²°ê³¼ ë¶„ì„
func (a *SimpleEOAAnalyzer) analyzeTransactionResult(tx *shareddomain.MarkedTransaction) {
	isDebug := false
	depositDetected := false

	// ì²˜ìŒ 5ê°œ íŠ¸ëœì­ì…˜ì˜ CEX ì²´í¬ ê³¼ì •ì„ ìì„¸íˆ ë¡œê¹…
	processedCount := atomic.LoadInt64(&a.stats.SuccessCount)

	// ì…ê¸ˆ ì£¼ì†Œ íƒì§€
	isCEX := a.infra.GroundKnowledge.IsCEXAddress(tx.To)
	if processedCount <= 5 {
		log.Printf("ğŸ” CEX Check #%d: To=%s â†’ IsCEX=%t",
			processedCount, tx.To.String(), isCEX)
	}

	if isCEX && isDebug {
		depositCount := atomic.AddInt64(&a.stats.DepositDetections, 1)
		depositDetected = true
		log.Printf("ğŸ¯ DEPOSIT DETECTED #%d: From: %s â†’ CEX: %s", depositCount, tx.From.String()[:10]+"...", tx.To.String()[:10]+"...")
	}

	// ê·¸ë˜í”„/ìœˆë„ìš° ì—…ë°ì´íŠ¸ ë¶„ë¥˜
	if a.infra.GroundKnowledge.IsDepositAddress(tx.To) && isDebug {
		graphCount := atomic.AddInt64(&a.stats.GraphUpdates, 1)
		log.Printf("ğŸ“Š GRAPH UPDATE #%d: From: %s â†’ Deposit: %s",
			graphCount, tx.From.String()[:10]+"...", tx.To.String()[:10]+"...")
	} else {

		windowCount := atomic.AddInt64(&a.stats.WindowUpdates, 1)
		if depositDetected && isDebug {
			log.Printf("ğŸ“ˆ WINDOW UPDATE #%d (with deposit): From: %s â†’ To: %s",
				windowCount, tx.From.String()[:10]+"...", tx.To.String()[:10]+"...")
		}
	}
}

// statsReporter ì£¼ê¸°ì  í†µê³„ ì¶œë ¥
func (a *SimpleEOAAnalyzer) statsReporter(ctx context.Context) {
	defer a.wg.Done()

	ticker := time.NewTicker(time.Duration(a.config.StatsInterval))
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-a.stopChannel:
			return
		case <-ticker.C:
			a.printStatistics()
		}
	}
}

// printStatistics í†µê³„ ì¶œë ¥
func (a *SimpleEOAAnalyzer) printStatistics() {
	total := atomic.LoadInt64(&a.stats.TotalProcessed)
	success := atomic.LoadInt64(&a.stats.SuccessCount)
	errors := atomic.LoadInt64(&a.stats.ErrorCount)
	deposits := atomic.LoadInt64(&a.stats.DepositDetections)
	graphUpdates := atomic.LoadInt64(&a.stats.GraphUpdates)
	windowUpdates := atomic.LoadInt64(&a.stats.WindowUpdates)
	dropped := atomic.LoadInt64(&a.stats.DroppedTxs)

	uptime := time.Since(a.stats.StartTime)
	channelUsage := len(a.infra.TxJobChannel)
	channelCapacity := cap(a.infra.TxJobChannel)
	usagePercent := float64(channelUsage) / float64(channelCapacity) * 100

	log.Printf("ğŸ“Š [%s] %s Statistics:", a.config.Mode, a.config.Name)
	log.Printf("   Uptime: %v | Processed: %d | Success: %d | Errors: %d",
		uptime.Round(time.Second), total, success, errors)
	log.Printf("   Deposits: %d | Graph: %d | Window: %d | Dropped: %d",
		deposits, graphUpdates, windowUpdates, dropped)
	log.Printf("   Channel: %d/%d (%.1f%%)", channelUsage, channelCapacity, usagePercent)

	if total > 0 {
		tps := float64(total) / uptime.Seconds()
		successRate := float64(success) / float64(total) * 100
		log.Printf("   Rate: %.1f tx/sec | Success Rate: %.1f%%", tps, successRate)
	}

	// DualManager í†µê³„
	if windowStats := a.dualManager.GetWindowStats(); windowStats != nil {
		log.Printf("   Buckets: %v | Pending: %v",
			windowStats["active_buckets"], windowStats["pending_relations"])
	}

	graphStats := a.relPool.RopeRepo.GetGraphStats()
	log.Printf("   Graph: %v nodes | %v edges",
		graphStats["total_nodes"], graphStats["total_edges"])

}

// GetStatistics í†µê³„ ë°˜í™˜
func (a *SimpleEOAAnalyzer) GetStatistics() map[string]any {
	return map[string]any{
		"mode":               string(a.config.Mode),
		"name":               a.config.Name,
		"total_processed":    atomic.LoadInt64(&a.stats.TotalProcessed),
		"success_count":      atomic.LoadInt64(&a.stats.SuccessCount),
		"error_count":        atomic.LoadInt64(&a.stats.ErrorCount),
		"deposit_detections": atomic.LoadInt64(&a.stats.DepositDetections),
		"graph_updates":      atomic.LoadInt64(&a.stats.GraphUpdates),
		"window_updates":     atomic.LoadInt64(&a.stats.WindowUpdates),
		"dropped_txs":        atomic.LoadInt64(&a.stats.DroppedTxs),
		"uptime_seconds":     time.Since(a.stats.StartTime).Seconds(),
		"channel_usage":      len(a.infra.TxJobChannel),
		"channel_capacity":   cap(a.infra.TxJobChannel),
	}
}

func (a *SimpleEOAAnalyzer) GetRopeDBStats() map[string]any {
	return a.relPool.RopeRepo.GetGraphStats()
}

// IsHealthy í—¬ìŠ¤ ìƒíƒœ ì²´í¬
func (a *SimpleEOAAnalyzer) IsHealthy() bool {
	total := atomic.LoadInt64(&a.stats.TotalProcessed)
	errors := atomic.LoadInt64(&a.stats.ErrorCount)

	if total == 0 {
		return true // ì•„ì§ íŠ¸ëœì­ì…˜ì´ ì—†ìœ¼ë©´ ê±´ê°•í•¨
	}

	errorRate := float64(errors) / float64(total)

	// ì—ëŸ¬ìœ¨ 10% ì´í•˜
	return errorRate < 0.1
}

// GetChannelStatus ì±„ë„ ìƒíƒœ ë°˜í™˜
func (a *SimpleEOAAnalyzer) GetChannelStatus() (int, int) {
	return len(a.infra.TxJobChannel), cap(a.infra.TxJobChannel)
}

// shutdown ìš°ì•„í•œ ì¢…ë£Œ
func (a *SimpleEOAAnalyzer) shutdown() error {
	log.Printf("ğŸ”„ Shutting down: %s", a.config.Name)

	// ì›Œì»¤í’€ ì¢…ë£Œ
	if a.infra.WorkerPool != nil {
		a.infra.WorkerPool.Shutdown()
		log.Printf("ğŸ”§ WorkerPool shutdown completed")
	}

	// ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸°
	done := make(chan struct{})
	go func() {
		a.wg.Wait()
		close(done)
	}()

	select {
	case <-done:
		log.Printf("âœ… All workers stopped gracefully")
	case <-time.After(10 * time.Second):
		log.Printf("âš ï¸ Shutdown timeout")
	}
	// í™œë™ì´ ì¢…ë£Œëœ ì±„ë„ì„ ë‹«ê¸°
	a.shutdownOnce.Do(func() {
		close(a.infra.TxJobChannel)
	})

	// ìµœì¢… í†µê³„ ì¶œë ¥
	if a.config.ResultReporting {
		a.printFinalReport()
	}
	a.printStatistics()

	// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
	if err := a.dualManager.Close(); err != nil {
		log.Printf("âš ï¸ Error closing dual manager: %v", err)
	}

	if err := a.relPool.RopeRepo.Close(); err != nil {
		log.Printf("âš ï¸ Error closing graph repository: %v", err)
	}

	// í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ ë°ì´í„° ì •ë¦¬
	if a.config.AutoCleanup {
		a.cleanup()
	}

	log.Printf("âœ… Shutdown completed: %s", a.config.Name)
	return nil
}

// printFinalReport ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥ (í…ŒìŠ¤íŠ¸ ëª¨ë“œìš©)
func (a *SimpleEOAAnalyzer) printFinalReport() {
	log.Printf("\n" + strings.Repeat("=", 80))
	log.Printf("ğŸ¯ FINAL REPORT: %s", a.config.Name)
	log.Printf(strings.Repeat("=", 80))

	total := atomic.LoadInt64(&a.stats.TotalProcessed)
	success := atomic.LoadInt64(&a.stats.SuccessCount)
	errors := atomic.LoadInt64(&a.stats.ErrorCount)
	deposits := atomic.LoadInt64(&a.stats.DepositDetections)
	graphUpdates := atomic.LoadInt64(&a.stats.GraphUpdates)
	windowUpdates := atomic.LoadInt64(&a.stats.WindowUpdates)
	dropped := atomic.LoadInt64(&a.stats.DroppedTxs)

	uptime := time.Since(a.stats.StartTime)

	log.Printf("ğŸ“Š Performance Summary:")
	log.Printf("   Total Runtime: %v", uptime.Round(time.Second))
	log.Printf("   Transactions Processed: %d", total)
	log.Printf("   Success Rate: %.2f%% (%d/%d)", float64(success)/float64(total)*100, success, total)
	log.Printf("   Processing Rate: %.1f tx/sec", float64(total)/uptime.Seconds())
	log.Printf("   Errors: %d | Dropped: %d", errors, dropped)

	log.Printf("\nğŸ” Analysis Results:")
	log.Printf("   Deposit Detections: %d", deposits)
	log.Printf("   Graph Updates: %d", graphUpdates)
	log.Printf("   Window Updates: %d", windowUpdates)

	// DualManager ìµœì¢… í†µê³„
	if windowStats := a.dualManager.GetWindowStats(); windowStats != nil {
		log.Printf("\nğŸªŸ Window Manager State:")
		for key, value := range windowStats {
			log.Printf("   %s: %v", key, value)
		}
	}

	graphStats := a.relPool.RopeRepo.GetGraphStats()
	log.Printf("\nğŸ—‚ï¸  Graph Database State:")
	for key, value := range graphStats {
		log.Printf("   %s: %v", key, value)
	}

	log.Printf(strings.Repeat("=", 80) + "\n")
}

// cleanup í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
func (a *SimpleEOAAnalyzer) cleanup() {
	if a.config.Mode != TestingMode {
		return
	}

	log.Printf("ğŸ§¹ Cleaning up test data: %s", a.config.IsolatedDBPath)

	if err := os.RemoveAll(a.config.IsolatedDBPath); err != nil {
		log.Printf("âš ï¸ Failed to cleanup test data: %v", err)
	} else {
		log.Printf("âœ… Test data cleaned up")
	}
}

// GetDualManager DualManager ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (API ì„œë²„ìš©)
func (a *SimpleEOAAnalyzer) GetDualManager() *DualManager {
	return a.dualManager
}

// Close io.Closer ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
func (a *SimpleEOAAnalyzer) Close() error {

	// Batch Consumer ì •ë¦¬
	if a.infra.BatchConsumer != nil {
		if err := a.infra.BatchConsumer.Close(); err != nil {
			log.Printf("âš ï¸ Error closing batch consumer: %v", err)
		}
	}

	return a.Stop()
}

// TODO ì¶”í›„ ì‚­ì œí•  ê²ƒ. ì–´ì©Œë‹¤ í”„ë¡œì„¸ìŠ¤ ë„ì¤‘ì— DBë°”ê¿€ ì¼ì´ ìˆê³ , í•˜í•„ ê·¸ê²Œ í…ŒìŠ¤íŠ¸ì½”ë“œë¼ ì¼ë‹¤ ë†”ë’€ìŒ
// TODO ì¶”í›„ ropeDBí…ŒìŠ¤íŠ¸ ë¦¬íŒ©í† ë§ í›„ ì œê±°í•  ê²ƒ
// !!í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„  ì ˆëŒ€ì ˆëŒ€ ì“°ì§€ ë§ê²„!!!
func (a *SimpleEOAAnalyzer) NullButAddDB(relPool *relapp.RelationPool) {
	a.relPool = relPool
}
