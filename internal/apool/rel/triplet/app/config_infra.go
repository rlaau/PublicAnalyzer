package app

import (
	"context"
	"fmt"
	"log"
	"strings"
	"time"

	"github.com/rlaaudgjs5638/chainAnalyzer/internal/apool/rel/triplet/infra"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/computation"
	shareddomain "github.com/rlaaudgjs5638/chainAnalyzer/shared/domain"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/eventbus"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/kafka"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/mode"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/workflow/workerpool"
)

func NewInfraByConfig(config *TripletConfig) *infra.TripletAndDualManagerInfra {
	var isolateColusre = computation.ComputeRelClosure(config.IsolatedDBPath)
	log.Printf("EOA Analyzerì˜ Infra ì„¸íŒ… ì¤‘: %s (Mode: %s)", config.Name, config.Mode)

	batchConsumer := loadKafkaBatchConsumer(config.Mode, config.Name)
	// ì›Œì»¤ í’€ì— ì“¸ ì±„ë„ ìƒì„±
	txJobBus, err := eventbus.NewWithPath[workerpool.Job](isolateColusre("workerPoolChan.jsonl"), config.BusCapLimit)
	if err != nil {
		panic("txJob EventBusìƒì„± ì¤‘ íŒ¨ë‹‰ ë°œìƒ")
	}
	//* ì›Œì»¤í’€ ìƒì„± ë° ì±„ë„ ë“±ë¡
	//*ì—¬ê¸°ì„  ctxë¥¼ ê·¸ëƒ¥ ë°±ê·¸ë¼ìš´ë“œë¡œ ë“±ë¡. ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ëŠ” Startì‹œ ê¸°ì¡´ ê²ƒ shuDowní›„ ë“±ë¡
	ctx := context.Background()
	workerPool := workerpool.New(ctx, config.WorkerCount, txJobBus)
	log.Printf("ğŸ”§ WorkerPool initialized with %d workers", config.WorkerCount)
	pendingDB, err := infra.NewBadgerPendingRelationRepo(isolateColusre("pending"))
	if err != nil {
		panic("íœë”œ ë ˆí¬ì§€í† ë¦¬ë¥¼ ì—´ì§€ ëª»í•¨.")
	}
	timeBucketManager, err := infra.NewTimeBucketManager(isolateColusre("timebucket"))
	if err != nil {
		panic("íƒ€ì„ë²„í‚· ë©”ë‹ˆì ¸ ë¡œë“œ ì¤‘ ì—ëŸ¬ ë°œìƒ")
	}
	fmt.Printf("Triplet ì¸í”„ë¼ ì „ë¶€ ë¡œë“œ ì™„ë£Œ")
	return infra.NewTripletInfra(txJobBus, workerPool, batchConsumer, pendingDB, timeBucketManager)

}

func loadKafkaBatchConsumer(mode mode.ProcessingMode, name string) *kafka.KafkaBatchConsumer[*shareddomain.MarkedTransaction] {
	// Transaction Consumer ì´ˆê¸°í™” - ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ í† í”½ ì‚¬ìš©
	kafkaBrokers := []string{kafka.DefaultKafkaPort}
	isTestMode := mode.IsTest()
	groupID := fmt.Sprintf("triplet-analyzer-%s", strings.ReplaceAll(name, " ", "-"))
	// ë°°ì¹˜ ëª¨ë“œ Consumer ì´ˆê¸°í™” (ê³ ì„±ëŠ¥)
	batchSize := 100                      // 100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
	batchTimeout := 20 * time.Millisecond // 20ms íƒ€ì„ì•„ì›ƒ
	var topic string
	if isTestMode {
		topic = kafka.TestFedTxTopic // í…ŒìŠ¤íŠ¸ìš© í† í”½
	} else {
		topic = kafka.ProductionTxTopic // í”„ë¡œë•ì…˜ìš© í† í”½
	}

	consumerConfig := kafka.KafkaBatchConfig{
		Brokers:      kafkaBrokers,
		Topic:        topic,
		GroupID:      groupID,
		BatchSize:    batchSize,
		BatchTimeout: batchTimeout,
	}
	batchConsumer := kafka.NewKafkaBatchConsumer[*shareddomain.MarkedTransaction](consumerConfig)
	log.Printf("ğŸ“¡ Batch consumer initialized (test mode: %v, batch size: %d)", isTestMode, batchSize)
	return batchConsumer
}

// func loadCEXSet(cexFilePath string) (*shareddomain.CEXSet, error) {
// 	if cexFilePath == "" {
// 		// ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš© (í›„ë°© í˜¸í™˜ì„±)
// 		cexFilePath = "internal/triplet/cex.txt"
// 	}
// 	cexRepo := infra.NewFileCEXRepository(cexFilePath)
// 	cexSet, err := cexRepo.LoadCEXSet()
// 	if err != nil {
// 		return nil, fmt.Errorf("failed to load CEX set from %s: %w", cexFilePath, err)
// 	}
// 	return cexSet, nil
// }
// func loadDetectedDepositSet(fileDBPath string, mode mode.ProcessingMode) (*infra.FileDepositRepository, error) {
// 	// ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
// 	if err := os.MkdirAll(fileDBPath, 0755); err != nil {
// 		return nil, fmt.Errorf("failed to create data directory: %w", err)
// 	}
// 	// Deposit ì €ì¥ì†Œ ì´ˆê¸°í™” - ëª¨ë“œì— ë”°ë¥¸ ê²½ë¡œ ì„¤ì •
// 	var detectedDepositFilePath string

// 	if mode.IsTest() {
// 		detectedDepositFilePath = fileDBPath + "/test_detected_deposits.csv"
// 	} else {
// 		detectedDepositFilePath = fileDBPath + "/production_detected_deposits.csv"
// 	}
// 	depositRepo := infra.NewFileDepositRepository(detectedDepositFilePath)
// 	return depositRepo, nil
// }
