package app

import (
	"context"
	"fmt"
	"log"
	"os"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/rlaaudgjs5638/chainAnalyzer/internal/ee/infra"
	shareddomain "github.com/rlaaudgjs5638/chainAnalyzer/shared/domain"
	"github.com/rlaaudgjs5638/chainAnalyzer/shared/kafka"
)

// SimpleEOAAnalyzer ê°„ë‹¨í•œ EOA ë¶„ì„ê¸° êµ¬í˜„ì²´
// * í…ŒìŠ¤íŠ¸ìš©ê³¼ í”„ë¡œë•ì…˜ìš© ëª¨ë‘ ì§€ì›í•˜ëŠ” ê¸°ë³¸ êµ¬í˜„
type SimpleEOAAnalyzer struct {
	// Core domain components
	dualManager *DualManager

	// WorkerPool integration
	//ë‚´ë¶€ ì±„ë„ì„
	stopChannel  chan struct{}
	stopOnce     sync.Once
	shutdownOnce sync.Once
	wg           sync.WaitGroup

	// Transaction consumer (Kafka ê¸°ë°˜)
	batchMode bool // ë°°ì¹˜ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€

	// Configuration
	config *EOAAnalyzerConfig

	// Statistics (thread-safe atomic counters)
	stats SimpleAnalyzerStats

	infra infra.TotalEOAAnalyzerInfra
}

// SimpleAnalyzerStats ê°„ë‹¨í•œ ë¶„ì„ê¸° í†µê³„
type SimpleAnalyzerStats struct {
	TotalProcessed    int64
	SuccessCount      int64
	ErrorCount        int64
	DepositDetections int64
	GraphUpdates      int64
	WindowUpdates     int64
	DroppedTxs        int64
	StartTime         time.Time
}

// NewProductionEOAAnalyzer í”„ë¡œë•ì…˜ìš© ë¶„ì„ê¸° ìƒì„±
func NewProductionEOAAnalyzer(config *EOAAnalyzerConfig, ctx context.Context) (EOAAnalyzer, error) {
	infraStructure := NewInfraByConfig(config, ctx)
	return newSimpleAnalyzer(config, infraStructure)
}

// NewTestingEOAAnalyzer í…ŒìŠ¤íŠ¸ìš© ë¶„ì„ê¸° ìƒì„±
func NewTestingEOAAnalyzer(config *EOAAnalyzerConfig, ctx context.Context) (EOAAnalyzer, error) {
	infraStructure := NewInfraByConfig(config, ctx)
	return newSimpleAnalyzer(config, infraStructure)
}

// newSimpleAnalyzer ê³µí†µ ë¶„ì„ê¸° ìƒì„± ë¡œì§
func newSimpleAnalyzer(config *EOAAnalyzerConfig, infraStructure infra.TotalEOAAnalyzerInfra) (*SimpleEOAAnalyzer, error) {
	//ì „ì²´ EOAì¸í”„ë¼ì—ì„œ êº¼ë‚´ ì“°ëŠ” í˜•ì‹
	dualManagerInfra := infra.NewDualManagerInfra(infraStructure.GroundKnowledge, infraStructure.GraphRepo, infraStructure.PendingRelationRepo)
	dualManager, err := NewDualManager(*dualManagerInfra)
	if err != nil {
		infraStructure.GraphRepo.Close()
		return nil, fmt.Errorf("failed to create dual manager: %w", err)
	}
	log.Printf("ğŸ”„ DualManager with pending DB at: %s", config.PendingDBPath)

	analyzer := &SimpleEOAAnalyzer{
		infra:       infraStructure,
		dualManager: dualManager,
		stopChannel: make(chan struct{}),
		batchMode:   true, // ê¸°ë³¸ê°’: ë°°ì¹˜ ëª¨ë“œ í™œì„±í™”
		config:      config,
		stats: SimpleAnalyzerStats{
			StartTime: time.Now(),
		},
	}

	log.Printf("âœ… Simple EOA Analyzer created: %s", config.Name)
	return analyzer, nil
}

// Start ë¶„ì„ê¸° ì‹œì‘
func (a *SimpleEOAAnalyzer) Start(ctx context.Context) error {
	log.Printf("ğŸš€ Starting Simple Analyzer: %s", a.config.Name)

	// Consumer ì‹œì‘ (ë°°ì¹˜ ëª¨ë“œ or ë‹¨ê±´ ëª¨ë“œ)
	if a.batchMode && a.infra.BatchConsumer != nil {
		// ë°°ì¹˜ ëª¨ë“œ: ë°°ì¹˜ Consumer ì‹œì‘
		a.wg.Add(1)
		go a.batchConsumerWorker(ctx)
		log.Printf("ğŸš€ Batch consumer started")
	} else {
		log.Printf("ë‹¨ê±´ ì»¨ìŠˆë¨¸ëŠ” ê± ì§€ì› ìŒ.")
	}

	// í†µê³„ ë¦¬í¬í„° ì‹œì‘
	a.wg.Add(1)
	go a.statsReporter(ctx)

	log.Printf("âœ… Simple Analyzer started: %s (%d workers + kafka consumer)", a.config.Name, a.config.WorkerCount)

	// ì»¨í…ìŠ¤íŠ¸ ì·¨ì†Œ ë˜ëŠ” ì •ì§€ ì‹œê·¸ë„ ëŒ€ê¸°
	select {
	case <-ctx.Done():
		log.Printf("ğŸ›‘ Context cancelled: %s", a.config.Name)
	case <-a.stopChannel:
		log.Printf("ğŸ›‘ Stop signal received: %s", a.config.Name)
	}

	return a.shutdown()
}

// Stop ë¶„ì„ê¸° ì¤‘ì§€
func (a *SimpleEOAAnalyzer) Stop() error {
	a.stopOnce.Do(func() {
		close(a.stopChannel)
	})
	return nil
}

// ProcessTransaction íŠ¸ëœì­ì…˜ ì²˜ë¦¬ (non-blocking)
func (a *SimpleEOAAnalyzer) ProcessTransaction(tx *shareddomain.MarkedTransaction) error {
	job := NewTransactionJob(tx, a, 0) // workerIDëŠ” ì›Œì»¤í’€ì—ì„œ ìë™ ê´€ë¦¬
	select {
	case a.infra.TxJobChannel <- job:
		return nil
	default:
		atomic.AddInt64(&a.stats.DroppedTxs, 1)
		return fmt.Errorf("channel full, dropped tx: %s", tx.TxID.String()[:8])
	}
}

// ProcessTransactions ë°°ì¹˜ íŠ¸ëœì­ì…˜ ì²˜ë¦¬
func (a *SimpleEOAAnalyzer) ProcessTransactions(txs []*shareddomain.MarkedTransaction) error {
	for _, tx := range txs {
		if err := a.ProcessTransaction(tx); err != nil {
			continue // ê°œë³„ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì²˜ë¦¬
		}
	}
	return nil
}

// transactionWorkerëŠ” ì´ì œ ì›Œì»¤í’€ì— ì˜í•´ ëŒ€ì²´ë¨ - í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì£¼ì„ ì²˜ë¦¬
// ì‹¤ì œ ì‘ì—…ì€ TransactionJob.Do()ì—ì„œ ì²˜ë¦¬ë¨

// batchConsumerWorker ë°°ì¹˜ Consumer ì›Œì»¤ (ê³ ì„±ëŠ¥ ë°°ì¹˜ ì²˜ë¦¬)
func (a *SimpleEOAAnalyzer) batchConsumerWorker(ctx context.Context) {
	defer a.wg.Done()

	log.Printf("ğŸš€ Batch consumer worker started")

	for {
		select {
		case <-ctx.Done():
			log.Printf("ğŸ›‘ Batch consumer worker stopping (context)")
			return
		case <-a.stopChannel:
			log.Printf("ğŸ›‘ Batch consumer worker stopping (signal)")
			return
		default:
			// ë°°ì¹˜ ë©”ì‹œì§€ ì½ê¸° (ë¸”ë¡œí‚¹)
			messages, err := a.infra.BatchConsumer.ReadMessagesBatch(ctx)
			if err != nil {
				// Context cancellationì€ ì •ìƒì ì¸ ì¢…ë£Œ
				if ctx.Err() != nil {
					return
				}
				// ê¸°íƒ€ ì—ëŸ¬ëŠ” ë¡œê¹…í•˜ê³  ê³„ì†
				log.Printf("âš ï¸ Batch read error: %v", err)
				time.Sleep(100 * time.Millisecond) // ì—ëŸ¬ ì‹œ ì§§ì€ ëŒ€ê¸°
				continue
			}

			// ë°°ì¹˜ê°€ ë¹„ì–´ìˆìœ¼ë©´ ìŠ¤í‚µ
			if len(messages) == 0 {
				continue
			}

			// ë°°ì¹˜ ì²˜ë¦¬ (ì§„ì •í•œ ë°°ì¹­!)
			a.processBatch(messages)
		}
	}
}

// processBatch ë°°ì¹˜ ë©”ì‹œì§€ ì²˜ë¦¬ (ê³ íš¨ìœ¨)
func (a *SimpleEOAAnalyzer) processBatch(messages []kafka.Message[*shareddomain.MarkedTransaction]) {
	batchSize := len(messages)
	processedCount := atomic.LoadInt64(&a.stats.TotalProcessed)

	// ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ë¡œê¹… (ì²˜ìŒ ëª‡ ë°°ì¹˜ë§Œ)
	if processedCount < 500 {
		log.Printf("ğŸ“¦ Processing batch of %d messages (total processed: %d)", batchSize, processedCount)
	}

	transactions := make([]*shareddomain.MarkedTransaction, 0, batchSize)

	// 1. ë©”ì‹œì§€ì—ì„œ ì§ì ‘ íŠ¸ëœì­ì…˜ ì¶”ì¶œ (íŒŒì‹± ë¶ˆí•„ìš”!)
	for _, msg := range messages {
		if msg.Value != nil {
			transactions = append(transactions, msg.Value)
		} else {
			atomic.AddInt64(&a.stats.ErrorCount, 1)
		}
	}

	// 2. íŠ¸ëœì­ì…˜ ì²˜ë¦¬ (ë°°ì¹˜ë¡œ ì²˜ë¦¬)
	for _, tx := range transactions {
		// ì›Œì»¤í’€ë¡œ ì‘ì—… ì „ë‹¬
		job := NewTransactionJob(tx, a, 0)
		select {
		case a.infra.TxJobChannel <- job:
			// ì„±ê³µ
		default:
			// ì±„ë„ì´ ê°€ë“ ì°¬ ê²½ìš° ë“œë¡­
			atomic.AddInt64(&a.stats.DroppedTxs, 1)
		}
	}

	// ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ë¡œê¹… (ì²˜ìŒ ëª‡ ë°°ì¹˜ë§Œ)
	if processedCount < 500 {
		log.Printf("ğŸ“¦ Batch processed: %d messages â†’ %d transactions", batchSize, len(transactions))
	}
}

// processSingleTransaction ë©”ì„œë“œëŠ” TransactionJob.Do()ë¡œ ì´ë™ë¨
// í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ì‚­ì œ

// analyzeTransactionResult íŠ¸ëœì­ì…˜ ê²°ê³¼ ë¶„ì„
func (a *SimpleEOAAnalyzer) analyzeTransactionResult(tx *shareddomain.MarkedTransaction) {
	depositDetected := false

	// ì²˜ìŒ 5ê°œ íŠ¸ëœì­ì…˜ì˜ CEX ì²´í¬ ê³¼ì •ì„ ìì„¸íˆ ë¡œê¹…
	processedCount := atomic.LoadInt64(&a.stats.SuccessCount)

	// ì…ê¸ˆ ì£¼ì†Œ íƒì§€
	isCEX := a.infra.GroundKnowledge.IsCEXAddress(tx.To)
	if processedCount <= 5 {
		log.Printf("ğŸ” CEX Check #%d: To=%s â†’ IsCEX=%t",
			processedCount, tx.To.String(), isCEX)
	}

	if isCEX {
		depositCount := atomic.AddInt64(&a.stats.DepositDetections, 1)
		depositDetected = true
		log.Printf("ğŸ¯ DEPOSIT DETECTED #%d: From: %s â†’ CEX: %s",
			depositCount, tx.From.String()[:10]+"...", tx.To.String()[:10]+"...")
	}

	// ê·¸ë˜í”„/ìœˆë„ìš° ì—…ë°ì´íŠ¸ ë¶„ë¥˜
	if a.infra.GroundKnowledge.IsDepositAddress(tx.To) {
		graphCount := atomic.AddInt64(&a.stats.GraphUpdates, 1)
		log.Printf("ğŸ“Š GRAPH UPDATE #%d: From: %s â†’ Deposit: %s",
			graphCount, tx.From.String()[:10]+"...", tx.To.String()[:10]+"...")
	} else {
		windowCount := atomic.AddInt64(&a.stats.WindowUpdates, 1)
		if depositDetected {
			log.Printf("ğŸ“ˆ WINDOW UPDATE #%d (with deposit): From: %s â†’ To: %s",
				windowCount, tx.From.String()[:10]+"...", tx.To.String()[:10]+"...")
		}
	}
}

// statsReporter ì£¼ê¸°ì  í†µê³„ ì¶œë ¥
func (a *SimpleEOAAnalyzer) statsReporter(ctx context.Context) {
	defer a.wg.Done()

	ticker := time.NewTicker(time.Duration(a.config.StatsInterval))
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			return
		case <-a.stopChannel:
			return
		case <-ticker.C:
			a.printStatistics()
		}
	}
}

// printStatistics í†µê³„ ì¶œë ¥
func (a *SimpleEOAAnalyzer) printStatistics() {
	total := atomic.LoadInt64(&a.stats.TotalProcessed)
	success := atomic.LoadInt64(&a.stats.SuccessCount)
	errors := atomic.LoadInt64(&a.stats.ErrorCount)
	deposits := atomic.LoadInt64(&a.stats.DepositDetections)
	graphUpdates := atomic.LoadInt64(&a.stats.GraphUpdates)
	windowUpdates := atomic.LoadInt64(&a.stats.WindowUpdates)
	dropped := atomic.LoadInt64(&a.stats.DroppedTxs)

	uptime := time.Since(a.stats.StartTime)
	channelUsage := len(a.infra.TxJobChannel)
	channelCapacity := cap(a.infra.TxJobChannel)
	usagePercent := float64(channelUsage) / float64(channelCapacity) * 100

	log.Printf("ğŸ“Š [%s] %s Statistics:", a.config.Mode, a.config.Name)
	log.Printf("   Uptime: %v | Processed: %d | Success: %d | Errors: %d",
		uptime.Round(time.Second), total, success, errors)
	log.Printf("   Deposits: %d | Graph: %d | Window: %d | Dropped: %d",
		deposits, graphUpdates, windowUpdates, dropped)
	log.Printf("   Channel: %d/%d (%.1f%%)", channelUsage, channelCapacity, usagePercent)

	if total > 0 {
		tps := float64(total) / uptime.Seconds()
		successRate := float64(success) / float64(total) * 100
		log.Printf("   Rate: %.1f tx/sec | Success Rate: %.1f%%", tps, successRate)
	}

	// DualManager í†µê³„
	if windowStats := a.dualManager.GetWindowStats(); windowStats != nil {
		log.Printf("   Buckets: %v | Pending: %v",
			windowStats["active_buckets"], windowStats["pending_relations"])
	}

	// Graph í†µê³„
	if graphStats, err := a.infra.GraphRepo.GetGraphStats(); err == nil {
		log.Printf("   Graph: %v nodes | %v edges",
			graphStats["total_nodes"], graphStats["total_edges"])
	}
}

// GetStatistics í†µê³„ ë°˜í™˜
func (a *SimpleEOAAnalyzer) GetStatistics() map[string]any {
	return map[string]any{
		"mode":               string(a.config.Mode),
		"name":               a.config.Name,
		"total_processed":    atomic.LoadInt64(&a.stats.TotalProcessed),
		"success_count":      atomic.LoadInt64(&a.stats.SuccessCount),
		"error_count":        atomic.LoadInt64(&a.stats.ErrorCount),
		"deposit_detections": atomic.LoadInt64(&a.stats.DepositDetections),
		"graph_updates":      atomic.LoadInt64(&a.stats.GraphUpdates),
		"window_updates":     atomic.LoadInt64(&a.stats.WindowUpdates),
		"dropped_txs":        atomic.LoadInt64(&a.stats.DroppedTxs),
		"uptime_seconds":     time.Since(a.stats.StartTime).Seconds(),
		"channel_usage":      len(a.infra.TxJobChannel),
		"channel_capacity":   cap(a.infra.TxJobChannel),
	}
}

// IsHealthy í—¬ìŠ¤ ìƒíƒœ ì²´í¬
func (a *SimpleEOAAnalyzer) IsHealthy() bool {
	total := atomic.LoadInt64(&a.stats.TotalProcessed)
	errors := atomic.LoadInt64(&a.stats.ErrorCount)

	if total == 0 {
		return true // ì•„ì§ íŠ¸ëœì­ì…˜ì´ ì—†ìœ¼ë©´ ê±´ê°•í•¨
	}

	channelUsage := float64(len(a.infra.TxJobChannel)) / float64(cap(a.infra.TxJobChannel))
	errorRate := float64(errors) / float64(total)

	// ì±„ë„ ì‚¬ìš©ë¥  90% ì´í•˜, ì—ëŸ¬ìœ¨ 10% ì´í•˜
	return channelUsage < 0.9 && errorRate < 0.1
}

// GetChannelStatus ì±„ë„ ìƒíƒœ ë°˜í™˜
func (a *SimpleEOAAnalyzer) GetChannelStatus() (int, int) {
	return len(a.infra.TxJobChannel), cap(a.infra.TxJobChannel)
}

// shutdown ìš°ì•„í•œ ì¢…ë£Œ
func (a *SimpleEOAAnalyzer) shutdown() error {
	log.Printf("ğŸ”„ Shutting down: %s", a.config.Name)

	// ì›Œì»¤í’€ ì¢…ë£Œ
	if a.infra.WorkerPool != nil {
		a.infra.WorkerPool.Shutdown()
		log.Printf("ğŸ”§ WorkerPool shutdown completed")
	}

	// ìƒˆ íŠ¸ëœì­ì…˜ ìˆ˜ì‹  ì¤‘ì§€ (í•œ ë²ˆë§Œ)
	a.shutdownOnce.Do(func() {
		close(a.infra.TxJobChannel)
	})

	// ëª¨ë“  ì›Œì»¤ ì™„ë£Œ ëŒ€ê¸°
	done := make(chan struct{})
	go func() {
		a.wg.Wait()
		close(done)
	}()

	select {
	case <-done:
		log.Printf("âœ… All workers stopped gracefully")
	case <-time.After(10 * time.Second):
		log.Printf("âš ï¸ Shutdown timeout")
	}

	// ìµœì¢… í†µê³„ ì¶œë ¥
	if a.config.ResultReporting {
		a.printFinalReport()
	}
	a.printStatistics()

	// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
	if err := a.dualManager.Close(); err != nil {
		log.Printf("âš ï¸ Error closing dual manager: %v", err)
	}

	if err := a.infra.GraphRepo.Close(); err != nil {
		log.Printf("âš ï¸ Error closing graph repository: %v", err)
	}

	// í…ŒìŠ¤íŠ¸ ëª¨ë“œì—ì„œ ë°ì´í„° ì •ë¦¬
	if a.config.AutoCleanup {
		a.cleanup()
	}

	log.Printf("âœ… Shutdown completed: %s", a.config.Name)
	return nil
}

// printFinalReport ìµœì¢… ë¦¬í¬íŠ¸ ì¶œë ¥ (í…ŒìŠ¤íŠ¸ ëª¨ë“œìš©)
func (a *SimpleEOAAnalyzer) printFinalReport() {
	log.Printf("\n" + strings.Repeat("=", 80))
	log.Printf("ğŸ¯ FINAL REPORT: %s", a.config.Name)
	log.Printf(strings.Repeat("=", 80))

	total := atomic.LoadInt64(&a.stats.TotalProcessed)
	success := atomic.LoadInt64(&a.stats.SuccessCount)
	errors := atomic.LoadInt64(&a.stats.ErrorCount)
	deposits := atomic.LoadInt64(&a.stats.DepositDetections)
	graphUpdates := atomic.LoadInt64(&a.stats.GraphUpdates)
	windowUpdates := atomic.LoadInt64(&a.stats.WindowUpdates)
	dropped := atomic.LoadInt64(&a.stats.DroppedTxs)

	uptime := time.Since(a.stats.StartTime)

	log.Printf("ğŸ“Š Performance Summary:")
	log.Printf("   Total Runtime: %v", uptime.Round(time.Second))
	log.Printf("   Transactions Processed: %d", total)
	log.Printf("   Success Rate: %.2f%% (%d/%d)", float64(success)/float64(total)*100, success, total)
	log.Printf("   Processing Rate: %.1f tx/sec", float64(total)/uptime.Seconds())
	log.Printf("   Errors: %d | Dropped: %d", errors, dropped)

	log.Printf("\nğŸ” Analysis Results:")
	log.Printf("   Deposit Detections: %d", deposits)
	log.Printf("   Graph Updates: %d", graphUpdates)
	log.Printf("   Window Updates: %d", windowUpdates)

	// DualManager ìµœì¢… í†µê³„
	if windowStats := a.dualManager.GetWindowStats(); windowStats != nil {
		log.Printf("\nğŸªŸ Window Manager State:")
		for key, value := range windowStats {
			log.Printf("   %s: %v", key, value)
		}
	}

	// Graph ìµœì¢… í†µê³„
	if graphStats, err := a.infra.GraphRepo.GetGraphStats(); err == nil {
		log.Printf("\nğŸ—‚ï¸  Graph Database State:")
		for key, value := range graphStats {
			log.Printf("   %s: %v", key, value)
		}
	}

	log.Printf(strings.Repeat("=", 80) + "\n")
}

// cleanup í…ŒìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬
func (a *SimpleEOAAnalyzer) cleanup() {
	if a.config.Mode != TestingMode {
		return
	}

	log.Printf("ğŸ§¹ Cleaning up test data: %s", a.config.FileDBPath)

	if err := os.RemoveAll(a.config.FileDBPath); err != nil {
		log.Printf("âš ï¸ Failed to cleanup test data: %v", err)
	} else {
		log.Printf("âœ… Test data cleaned up")
	}
}

// Close io.Closer ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
func (a *SimpleEOAAnalyzer) Close() error {

	// Batch Consumer ì •ë¦¬
	if a.infra.BatchConsumer != nil {
		if err := a.infra.BatchConsumer.Close(); err != nil {
			log.Printf("âš ï¸ Error closing batch consumer: %v", err)
		}
	}

	return a.Stop()
}
